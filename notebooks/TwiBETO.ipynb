{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BETO\n",
    "\n",
    "In this notebook, we will check what happens if we fine tune using MLM on the TASS tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4802 2443 7264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Index(['text', 'polarity', 'label'], dtype='object'),\n",
       " Index(['text', 'polarity', 'label'], dtype='object'),\n",
       " Index(['text', 'polarity', 'label'], dtype='object'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def get_lang(file):\n",
    "    return os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "\"\"\"\n",
    "Lo pongo as√≠ por hugginface\n",
    "\"\"\"\n",
    "id2label = {0: 'N', 1: 'NEU', 2: 'P'}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "def load_df(file):\n",
    "    dialect = get_lang(file)\n",
    "    \n",
    "    df = pd.read_table(file, names=[\"id\", \"text\", \"polarity\"], index_col=0)\n",
    "    #df[\"dialect\"] = dialect\n",
    "    \n",
    "    for label, idx in label2id.items():\n",
    "        df.loc[df[\"polarity\"] == label, \"label\"] = idx\n",
    "    return df\n",
    "\n",
    "train_files = glob(\"../data/tass2020/train/*.tsv\")\n",
    "dev_files = glob(\"../data/tass2020/dev/*.tsv\")\n",
    "test_files = glob(\"../data/tass2020/test1.1/*.tsv\")\n",
    "\n",
    "train_dfs = {get_lang(file):load_df(file) for file in train_files}\n",
    "dev_dfs = {get_lang(file):load_df(file) for file in dev_files}\n",
    "test_dfs = {get_lang(file):load_df(file) for file in test_files}\n",
    "\n",
    "train_df = pd.concat(train_dfs.values())\n",
    "dev_df = pd.concat(dev_dfs.values())\n",
    "test_df = pd.concat(test_dfs.values())\n",
    "\n",
    "print(len(train_df), len(dev_df), len(test_df))\n",
    "\n",
    "train_df.columns, dev_df.columns, test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ce8b0a9fbf46729736ff262c8a6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242120.0, style=ProgressStyle(descripti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb24277172b1470bb3136cdd7a5a4f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673c208133084e75a2ce0911178d26fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4b2392d54f44e3bb8c7f000656decb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=43.0, style=ProgressStyle(description_w‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(model_name, return_dict=True, num_labels=3)\n",
    "model = model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "with open(\"tweets.txt\", \"w\") as f:\n",
    "    for tweet in train_df[\"text\"]:\n",
    "        f.write(preprocess_tweet(tweet) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default-617ef339ebbfa8ff (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/jmperez/.cache/huggingface/datasets/text/default-617ef339ebbfa8ff/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/jmperez/.cache/huggingface/datasets/text/default-617ef339ebbfa8ff/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.\n",
      "CPU times: user 132 ms, sys: 4 ms, total: 136 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"./tweets.txt\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** ARREGLAR ESTO DE ACA ARRIBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmperez/.local/share/virtualenvs/pysent-oyXQVI9B/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:110: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 4 ms, total: 1.74 s\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./tweets.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./TwiBETO\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=30,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2280/2280 12:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.431588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.208357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.036722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.947806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 47s, sys: 2min 53s, total: 12min 40s\n",
      "Wall time: 12min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2280, training_loss=1.1238509328741777)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./TwiBETO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./TwiBETO and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "\n",
    "fill_mask_beto = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name\n",
    ")\n",
    "\n",
    "fill_mask_twibeto = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./TwiBETO\",\n",
    "    tokenizer=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Spinetta es un futbolista [SEP]',\n",
       "  'score': 0.00017552149074617773,\n",
       "  'token': 12790,\n",
       "  'token_str': 'futbolista'},\n",
       " {'sequence': '[CLS] Spinetta es un m√∫sico [SEP]',\n",
       "  'score': 4.034613539261045e-06,\n",
       "  'token': 14568,\n",
       "  'token_str': 'm√∫sico'},\n",
       " {'sequence': '[CLS] Spinetta es un pol√≠tico [SEP]',\n",
       "  'score': 1.456219479223364e-06,\n",
       "  'token': 4593,\n",
       "  'token_str': 'pol√≠tico'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"Spinetta es un [MASK]\"\n",
    "\n",
    "fill_mask_twibeto(phrase, targets=[\"m√∫sico\", \"futbolista\", \"pol√≠tico\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] Spinetta es un m√∫sico [SEP]',\n",
       "  'score': 0.03277350589632988,\n",
       "  'token': 14568,\n",
       "  'token_str': 'm√∫sico'},\n",
       " {'sequence': '[CLS] Spinetta es un pol√≠tico [SEP]',\n",
       "  'score': 0.0009327398729510605,\n",
       "  'token': 4593,\n",
       "  'token_str': 'pol√≠tico'},\n",
       " {'sequence': '[CLS] Spinetta es un futbolista [SEP]',\n",
       "  'score': 0.0005494855577126145,\n",
       "  'token': 12790,\n",
       "  'token_str': 'futbolista'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_beto(phrase, targets=[\"m√∫sico\", \"futbolista\", \"pol√≠tico\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mfill_mask_beto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           FillMaskPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.FillMaskPipeline object at 0x7f59817fa160>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/pysent-oyXQVI9B/lib/python3.8/site-packages/transformers/pipelines.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Masked language modeling prediction pipeline using any :obj:`ModelWithLMHead`. See the `masked language modeling\n",
       "examples <../task_summary.html#masked-language-modeling>`__ for more information.\n",
       "\n",
       "This mask filling pipeline can currently be loaded from :func:`~transformers.pipeline` using the following task\n",
       "identifier: :obj:`\"fill-mask\"`.\n",
       "\n",
       "The models that this pipeline can use are models that have been trained with a masked language modeling objective,\n",
       "which includes the bi-directional models in the library. See the up-to-date list of available models on\n",
       "`huggingface.co/models <https://huggingface.co/models?filter=masked-lm>`__.\n",
       "\n",
       ".. note::\n",
       "\n",
       "    This pipeline only works for inputs with exactly one token masked.\n",
       "\n",
       "Arguments:\n",
       "    model (:obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`):\n",
       "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
       "        :class:`~transformers.PreTrainedModel` for PyTorch and :class:`~transformers.TFPreTrainedModel` for\n",
       "        TensorFlow.\n",
       "    tokenizer (:obj:`~transformers.PreTrainedTokenizer`):\n",
       "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
       "        :class:`~transformers.PreTrainedTokenizer`.\n",
       "    modelcard (:obj:`str` or :class:`~transformers.ModelCard`, `optional`):\n",
       "        Model card attributed to the model for this pipeline.\n",
       "    framework (:obj:`str`, `optional`):\n",
       "        The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
       "        must be installed.\n",
       "\n",
       "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
       "        both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no model\n",
       "        is provided.\n",
       "    task (:obj:`str`, defaults to :obj:`\"\"`):\n",
       "        A task-identifier for the pipeline.\n",
       "    args_parser (:class:`~transformers.pipelines.ArgumentHandler`, `optional`):\n",
       "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
       "    device (:obj:`int`, `optional`, defaults to -1):\n",
       "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
       "        the associated CUDA device id.\n",
       "    binary_output (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
       "\n",
       "    top_k (:obj:`int`, defaults to 5): The number of predictions to return.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Fill the masked token in the text(s) given as inputs.\n",
       "\n",
       "Args:\n",
       "    args (:obj:`str` or :obj:`List[str]`):\n",
       "        One or several texts (or one list of prompts) with masked tokens.\n",
       "    targets (:obj:`str` or :obj:`List[str]`, `optional`):\n",
       "        When passed, the model will return the scores for the passed token or tokens rather than the top k\n",
       "        predictions in the entire vocabulary. If the provided targets are not in the model vocab, they will be\n",
       "        tokenized and the first resulting token will be used (with a warning).\n",
       "    top_k (:obj:`int`, `optional`):\n",
       "        When passed, overrides the number of predictions to return.\n",
       "\n",
       "Return:\n",
       "    A list or a list of list of :obj:`dict`: Each result comes as list of dictionaries with the following keys:\n",
       "\n",
       "    - **sequence** (:obj:`str`) -- The corresponding input with the mask token prediction.\n",
       "    - **score** (:obj:`float`) -- The corresponding probability.\n",
       "    - **token** (:obj:`int`) -- The predicted token id (to replace the masked one).\n",
       "    - **token** (:obj:`str`) -- The predicted token (to replace the masked one).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fill_mask_beto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
